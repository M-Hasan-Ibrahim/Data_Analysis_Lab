{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce54fe1f-9c0a-4006-884d-6621692f7ae3",
   "metadata": {},
   "source": [
    "# Lab 03 : Advanced Numerical and Categorical Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60409b0-3e67-4d0a-a70c-4024d9b0cc79",
   "metadata": {},
   "source": [
    "#### Lab Overview\n",
    "\n",
    "This workshop focuses on using a processed tabular dataset to extract usefule features, build an optimized model, and evaluate its performance using advanced techniques.\n",
    "\n",
    "---\n",
    "\n",
    "#### Objective\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Select and split data\n",
    "2. Select an ML model and perform hyperparameter tuning\n",
    "3. Test and evaluate the trained model\n",
    "4. apply explainability methods to interpret model decisions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753d735-3610-4ac3-ad3c-f099a68fbfd6",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "\n",
    "**How to split**<br>\n",
    "The primary goal of data splitting is to ensure that the model you build is evaluated fairly and can generalize to new, unseen data. You split the data into three main subsets:\n",
    "\n",
    "- Training Set: used for model training and consists of 60%-80% of the total data\n",
    "- Validation Set: used for model tuning and consists of 10%-20% of the total data\n",
    "- Test Set: used for model evaluation and consists of 10%-20% of the total data\n",
    "\n",
    "**Best practices**\n",
    "\n",
    "- Randomize Data: Always shuffle the data before splitting to ensure the splits are representative of the entire dataset.\n",
    "- Stratify for Imbalanced Data: For classification tasks with imbalanced classes, stratify your data to ensure that the proportions of classes in your splits are similar to the full dataset.\n",
    "- Avoid Data Leakage: Ensure that the test set remains unseen throughout the training and validation process. The model should never have access to any data in the test set before final evaluation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d70f09-c119-44c5-abc5-414217a12e32",
   "metadata": {},
   "source": [
    "#### Model Selection\n",
    "\n",
    "Below is an overview of common classification and regression algorithms, including brief descriptions, when to use each algorithm, and key parameters that can be tuned to optimize performance. By adjusting these parameters, you can find the best model for your dataset and task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01588d",
   "metadata": {},
   "source": [
    "##### Classification Algorims\n",
    "\n",
    "| Algorithm                        | Description                                                                                          | When to Use                                                                                                                      | Parameters to Tune                                                                                                                      |\n",
    "| -------------------------------- | ---------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Logistic Regression**          | A statistical method for binary classification, based on the logistic function.                      | When the output is binary (0/1) or multi-class (1-vs-rest). Suitable for linear decision boundaries.                             | - **C** (Regularization strength)<br>- **solver** (Algorithm to use for optimization)<br>- **max_iter** (Number of iterations)          |\n",
    "| **K-Nearest Neighbors (KNN)**    | A non-parametric algorithm that classifies based on the majority class of nearest neighbors.         | When the data has many classes, or non-linear decision boundaries. Suitable for small datasets with well-defined boundaries.     | - **n_neighbors** (Number of neighbors)<br>- **weights** (Distance weighting)<br>- **metric** (Distance metric)                         |\n",
    "| **Support Vector Machine (SVM)** | A supervised learning model that finds the hyperplane that best separates classes.                   | When you need high-dimensional space separation and robust decision boundaries.                                                  | - **C** (Regularization parameter)<br>- **kernel** (Linear, RBF, etc.)<br>- **gamma** (Kernel coefficient)                              |\n",
    "| **Random Forest**                | An ensemble method that creates multiple decision trees and combines their predictions.              | When you have large datasets with high dimensionality and feature interactions. Works well for imbalanced data.                  | - **n_estimators** (Number of trees)<br>- **max_depth** (Max depth of each tree)<br>- **min_samples_split** (Minimum samples for split) |\n",
    "| **Decision Tree**                | A tree-based algorithm that splits data into subsets to make decisions based on feature values.      | When the model needs to be interpretable or easy to visualize. Can handle both classification and regression tasks.              | - **max_depth** (Max depth of the tree)<br>- **min_samples_split** (Minimum samples for split)<br>- **criterion** (Split quality)       |\n",
    "| **Naive Bayes**                  | A probabilistic classifier based on Bayes' Theorem with strong independence assumptions.             | When features are independent and you need a fast and simple model. Works well with categorical data.                            | - **alpha** (Laplace smoothing)<br>- **fit_prior** (Whether to learn class prior probabilities)                                         |\n",
    "| **Gradient Boosting (GBDT)**     | An ensemble method that builds decision trees sequentially, optimizing for errors of previous trees. | When you need robust performance and are willing to trade off model complexity for higher accuracy. Suitable for large datasets. | - **n_estimators** (Number of trees)<br>- **learning_rate** (Step size)<br>- **max_depth** (Tree depth)                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48d8ea",
   "metadata": {},
   "source": [
    "##### Regression Algorithms\n",
    "\n",
    "| Algorithm                           | Description                                                                                                                           | When to Use                                                                                                     | Parameters to Tune                                                                                                                      |\n",
    "| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Linear Regression**               | A linear approach for modeling the relationship between a dependent variable and one or more independent variables.                   | When the relationship between input features and the target is linear.                                          | - **fit_intercept** (Whether to include an intercept term)<br>- **normalize** (Whether to normalize the input data)                     |\n",
    "| **Decision Tree Regression**        | A tree-based algorithm that predicts continuous values by splitting data based on feature values.                                     | When relationships between features are non-linear or when interpretability is important.                       | - **max_depth** (Max depth of the tree)<br>- **min_samples_split** (Minimum samples for split)<br>- **criterion** (Split quality)       |\n",
    "| **Random Forest Regression**        | An ensemble method using multiple decision trees to make predictions and average their results.                                       | When there are complex feature interactions and non-linear relationships. Works well for high-dimensional data. | - **n_estimators** (Number of trees)<br>- **max_depth** (Max depth of each tree)<br>- **min_samples_split** (Minimum samples for split) |\n",
    "| **Support Vector Regression (SVR)** | A regression method based on the principles of Support Vector Machine (SVM), which tries to fit the error within a certain threshold. | When the data is noisy and you need robust regression with high-dimensional data.                               | - **C** (Regularization parameter)<br>- **kernel** (Linear, RBF, etc.)<br>- **epsilon** (Epsilon for the margin of error)               |\n",
    "| **K-Nearest Neighbors Regression**  | A non-parametric regression method that predicts values based on the average of k nearest neighbors.                                  | When the relationship between features and target is complex, and there's no clear functional form.             | - **n_neighbors** (Number of neighbors)<br>- **weights** (Distance weighting)<br>- **metric** (Distance metric)                         |\n",
    "| **Gradient Boosting Regression**    | An ensemble method that builds models sequentially, focusing on correcting errors made by previous models.                            | When you need high accuracy for regression problems and are willing to handle more complex models.              | - **n_estimators** (Number of trees)<br>- **learning_rate** (Step size)<br>- **max_depth** (Tree depth)                                 |\n",
    "| **Lasso Regression**                | A linear model that uses L1 regularization to penalize the absolute value of coefficients, helping with feature selection.            | When there is multicollinearity in the dataset, or you need to perform feature selection.                       | - **alpha** (Regularization strength)                                                                                                   |\n",
    "| **Ridge Regression**                | Similar to Lasso but with L2 regularization, penalizing the square of the coefficients.                                               | When you need to prevent overfitting but don't need feature selection like Lasso.                               | - **alpha** (Regularization strength)                                                                                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f9111",
   "metadata": {},
   "source": [
    "##### Model Training and Hyperparameter tuning\n",
    "\n",
    "**Training Options:**\n",
    "\n",
    "- Train Base Model, Then Fine-tune Using Validation Set:Train a base model on the training set, then fine-tune it using the validation set to improve performance and adjust hyperparameters.\n",
    "- Train Multiple Models on Entire Dataset: Train several models with different parameters on the full dataset without a validation set, comparing their performance.\n",
    "\n",
    "**Search Options:**\n",
    "\n",
    "- Grid Search: Exhaustively tests all combinations of hyperparameters.\n",
    "- Random Search: Randomly selects hyperparameter combinations from the search space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a73dc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Model Evaluation\n",
    "\n",
    "Each type of model has its own set of metrics to evaluate its performance. The model is assessed by predicting the output for the test data and then calculating the metrics by comparing the predictions to the actual outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55185f",
   "metadata": {},
   "source": [
    "##### Classification Model Evaluation Metrics\n",
    "\n",
    "| **Metric**               | **Description**                                                                                                       | **When to Use**                                                                             | **How to Identify if the Model is Good**                                                             |\n",
    "| ------------------------ | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| **Accuracy**             | The proportion of correctly predicted instances out of the total instances.                                           | When the classes are balanced or when you just need a quick measure of overall performance. | High accuracy (close to 1) indicates a good model, but may be misleading with imbalanced data.       |\n",
    "| **Precision**            | The proportion of true positive predictions out of all positive predictions made by the model.                        | Useful when false positives are costly (e.g., spam detection).                              | High precision (close to 1) means the model has few false positives.                                 |\n",
    "| **Recall (Sensitivity)** | The proportion of true positives detected out of all actual positive instances.                                       | Useful when false negatives are costly (e.g., medical diagnoses).                           | High recall (close to 1) indicates that most actual positives are identified.                        |\n",
    "| **F1-Score**             | The harmonic mean of precision and recall. Balances both metrics, providing a single score for performance.           | When both precision and recall are important to balance, especially in imbalanced datasets. | A high F1-score (close to 1) suggests a good balance between precision and recall.                   |\n",
    "| **AUC-ROC Curve**        | The area under the Receiver Operating Characteristic curve, showing the tradeoff between sensitivity and specificity. | Useful to evaluate model performance for binary classification across various thresholds.   | AUC closer to 1 indicates a better model, with 0.5 meaning no discrimination ability.                |\n",
    "| **Confusion Matrix**     | A matrix showing true positives, true negatives, false positives, and false negatives.                                | Provides detailed insight into classification errors, especially in imbalanced datasets.    | Good models will have high true positives and true negatives, and low false positives and negatives. |\n",
    "| **Log Loss**             | Measures the performance of classification models where the prediction is a probability value.                        | When working with probabilistic models, especially in multi-class classification.           | Lower log loss values (close to 0) indicate better predictive accuracy.                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195ae2c",
   "metadata": {},
   "source": [
    "##### Regression Model Evaluation Metrics\n",
    "\n",
    "| **Metric**                                | **Description**                                                                                 | **When to Use**                                                                | **How to Identify if the Model is Good**                                                  |\n",
    "| ----------------------------------------- | ----------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |\n",
    "| **Mean Absolute Error (MAE)**             | The average of the absolute differences between the predicted and actual values.                | When you want a simple and interpretable error metric.                         | Lower MAE values indicate better model performance, with 0 being perfect.                 |\n",
    "| **Mean Squared Error (MSE)**              | The average of the squared differences between the predicted and actual values.                 | When penalizing larger errors is more important than smaller ones.             | Lower MSE values indicate better model performance.                                       |\n",
    "| **Root Mean Squared Error (RMSE)**        | The square root of the mean squared error, providing the error in the same units as the target. | When you want to emphasize larger errors.                                      | Lower RMSE values suggest better model performance, with 0 being perfect.                 |\n",
    "| **R-squared (R²)**                        | The proportion of variance in the dependent variable explained by the model.                    | When you want to measure how well the model explains the variance in the data. | R² close to 1 indicates a good fit, but very high values may indicate overfitting.        |\n",
    "| **Adjusted R-squared**                    | Adjusted version of R-squared that accounts for the number of predictors in the model.          | When you want to compare models with different numbers of features.            | Higher values indicate better model fit after accounting for complexity.                  |\n",
    "| **Mean Absolute Percentage Error (MAPE)** | Measures the average percentage error between predicted and actual values.                      | When dealing with percentage errors and need an interpretable metric.          | Lower MAPE values (below 10% is often considered good) indicate better model performance. |\n",
    "| **Explained Variance Score**              | Measures the proportion of variance explained by the model.                                     | When you want to know how much of the data variance is captured by the model.  | Higher values closer to 1 indicate a better model.                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51699df1",
   "metadata": {},
   "source": [
    "#### Hands-on Activity\n",
    "\n",
    "For the output dataset from Lab 2, perform the following tasks:\n",
    "\n",
    "**Task 1**: Based on the statistical tests performed in Lab 2, provide a list of relevant features to be used in the model. If needed, conduct additional statistical tests. Explain your reasoning for selecting or dropping features.\n",
    "\n",
    "_You now have two datasets: one with all features and one with selected features. Apply the following tasks to both datasets._\n",
    "\n",
    "**Task 2**: Split both datasets into training, validation (optional), and test sets, following a reasonable ratio that aligns with best practices. Justify the proportions and approach you used for the split.\n",
    "\n",
    "**Task 3**: For each dataset, select two algorithms, build models, train them, and tune hyperparameters using grid search or random search. Explain your rationale behind your choices.\n",
    "\n",
    "**Task 4**: Evaluate the models on the test set using appropriate metrics for the type of problem.\n",
    "\n",
    "**Task 5**: Compare the performance of the models on both the full-feature dataset and the feature-selected dataset.\n",
    "\n",
    "**Task 6**: Analyze the impact of feature selection on model performance. Provide insights into whether feature selection improved the models and why.\n",
    "\n",
    "**Task 7**: Conclude which dataset, model, and hyperparameters performed the best, based on the evaluation results.\n",
    "\n",
    "##### Note the following:\n",
    "\n",
    "- When necessary display/add briefly the logic/reasoning of a data procedure done.\n",
    "- Write clean code, allocate at least 1 code block for each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9cac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c19481c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>native-continent</th>\n",
       "      <th>race-num</th>\n",
       "      <th>sex-num</th>\n",
       "      <th>income-num</th>\n",
       "      <th>fnlwgt-normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.052010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.055898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>3</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.144690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>Some High School</td>\n",
       "      <td>2</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.157489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.227059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.144538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>3</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.215649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.251599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.056288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.122214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt         education  education-num  \\\n",
       "0       39         State-gov   77516        Bachelor's              6   \n",
       "1       50  Self-emp-not-inc   83311        Bachelor's              6   \n",
       "2       38           Private  215646           HS-grad              3   \n",
       "3       53           Private  234721  Some High School              2   \n",
       "4       28           Private  338409        Bachelor's              6   \n",
       "...    ...               ...     ...               ...            ...   \n",
       "48837   39           Private  215419        Bachelor's              6   \n",
       "48838   64           Unknown  321403           HS-grad              3   \n",
       "48839   38           Private  374983        Bachelor's              6   \n",
       "48840   44           Private   83891        Bachelor's              6   \n",
       "48841   35      Self-emp-inc  182148        Bachelor's              6   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           Never-married       Adm-clerical   Not-in-family   \n",
       "1      Married-civ-spouse    Exec-managerial         Husband   \n",
       "2                Divorced  Handlers-cleaners   Not-in-family   \n",
       "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
       "4      Married-civ-spouse     Prof-specialty            Wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            Divorced     Prof-specialty   Not-in-family   \n",
       "48838             Widowed            Unknown  Other-relative   \n",
       "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
       "48840            Divorced       Adm-clerical       Own-child   \n",
       "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   White    Male          2174             0              40   \n",
       "1                   White    Male             0             0              13   \n",
       "2                   White    Male             0             0              40   \n",
       "3                   Black    Male             0             0              40   \n",
       "4                   Black  Female             0             0              40   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               White  Female             0             0              36   \n",
       "48838               Black    Male             0             0              40   \n",
       "48839               White    Male             0             0              50   \n",
       "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
       "48841               White    Male             0             0              60   \n",
       "\n",
       "      native-country income native-continent  race-num  sex-num  income-num  \\\n",
       "0      United-States  <=50K    North America         1        1          -1   \n",
       "1      United-States  <=50K    North America         1        1          -1   \n",
       "2      United-States  <=50K    North America         1        1          -1   \n",
       "3      United-States  <=50K    North America         2        1          -1   \n",
       "4               Cuba  <=50K    North America         2        0          -1   \n",
       "...              ...    ...              ...       ...      ...         ...   \n",
       "48837  United-States  <=50K    North America         1        0          -1   \n",
       "48838  United-States  <=50K    North America         2        1          -1   \n",
       "48839  United-States  <=50K    North America         1        1          -1   \n",
       "48840  United-States  <=50K    North America         3        1          -1   \n",
       "48841  United-States   >50K    North America         1        1           1   \n",
       "\n",
       "       fnlwgt-normalized  \n",
       "0               0.052010  \n",
       "1               0.055898  \n",
       "2               0.144690  \n",
       "3               0.157489  \n",
       "4               0.227059  \n",
       "...                  ...  \n",
       "48837           0.144538  \n",
       "48838           0.215649  \n",
       "48839           0.251599  \n",
       "48840           0.056288  \n",
       "48841           0.122214  \n",
       "\n",
       "[48842 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   age                48842 non-null  int64  \n",
      " 1   workclass          48842 non-null  object \n",
      " 2   fnlwgt             48842 non-null  int64  \n",
      " 3   education          48842 non-null  object \n",
      " 4   education-num      48842 non-null  int64  \n",
      " 5   marital-status     48842 non-null  object \n",
      " 6   occupation         48842 non-null  object \n",
      " 7   relationship       48842 non-null  object \n",
      " 8   race               48842 non-null  object \n",
      " 9   sex                48842 non-null  object \n",
      " 10  capital-gain       48842 non-null  int64  \n",
      " 11  capital-loss       48842 non-null  int64  \n",
      " 12  hours-per-week     48842 non-null  int64  \n",
      " 13  native-country     48842 non-null  object \n",
      " 14  income             48842 non-null  object \n",
      " 15  native-continent   48842 non-null  object \n",
      " 16  race-num           48842 non-null  int64  \n",
      " 17  sex-num            48842 non-null  int64  \n",
      " 18  income-num         48842 non-null  int64  \n",
      " 19  fnlwgt-normalized  48842 non-null  float64\n",
      "dtypes: float64(1), int64(9), object(10)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data\\lab_02_A_cleaned.csv\")\n",
    "display(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56b86f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-continent</th>\n",
       "      <th>race-num</th>\n",
       "      <th>sex-num</th>\n",
       "      <th>income-num</th>\n",
       "      <th>fnlwgt-normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.052010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.055898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>3</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.144690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>2</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.157489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.227059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>6</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.144538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>321403</td>\n",
       "      <td>3</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.215649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.251599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>6</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>North America</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.056288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.122214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass  fnlwgt  education-num      marital-status  \\\n",
       "0       39         State-gov   77516              6       Never-married   \n",
       "1       50  Self-emp-not-inc   83311              6  Married-civ-spouse   \n",
       "2       38           Private  215646              3            Divorced   \n",
       "3       53           Private  234721              2  Married-civ-spouse   \n",
       "4       28           Private  338409              6  Married-civ-spouse   \n",
       "...    ...               ...     ...            ...                 ...   \n",
       "48837   39           Private  215419              6            Divorced   \n",
       "48838   64           Unknown  321403              3             Widowed   \n",
       "48839   38           Private  374983              6  Married-civ-spouse   \n",
       "48840   44           Private   83891              6            Divorced   \n",
       "48841   35      Self-emp-inc  182148              6  Married-civ-spouse   \n",
       "\n",
       "              occupation    relationship                race  capital-gain  \\\n",
       "0           Adm-clerical   Not-in-family               White          2174   \n",
       "1        Exec-managerial         Husband               White             0   \n",
       "2      Handlers-cleaners   Not-in-family               White             0   \n",
       "3      Handlers-cleaners         Husband               Black             0   \n",
       "4         Prof-specialty            Wife               Black             0   \n",
       "...                  ...             ...                 ...           ...   \n",
       "48837     Prof-specialty   Not-in-family               White             0   \n",
       "48838            Unknown  Other-relative               Black             0   \n",
       "48839     Prof-specialty         Husband               White             0   \n",
       "48840       Adm-clerical       Own-child  Asian-Pac-Islander          5455   \n",
       "48841    Exec-managerial         Husband               White             0   \n",
       "\n",
       "       capital-loss  hours-per-week native-continent  race-num  sex-num  \\\n",
       "0                 0              40    North America         1        1   \n",
       "1                 0              13    North America         1        1   \n",
       "2                 0              40    North America         1        1   \n",
       "3                 0              40    North America         2        1   \n",
       "4                 0              40    North America         2        0   \n",
       "...             ...             ...              ...       ...      ...   \n",
       "48837             0              36    North America         1        0   \n",
       "48838             0              40    North America         2        1   \n",
       "48839             0              50    North America         1        1   \n",
       "48840             0              40    North America         3        1   \n",
       "48841             0              60    North America         1        1   \n",
       "\n",
       "       income-num  fnlwgt-normalized  \n",
       "0              -1           0.052010  \n",
       "1              -1           0.055898  \n",
       "2              -1           0.144690  \n",
       "3              -1           0.157489  \n",
       "4              -1           0.227059  \n",
       "...           ...                ...  \n",
       "48837          -1           0.144538  \n",
       "48838          -1           0.215649  \n",
       "48839          -1           0.251599  \n",
       "48840          -1           0.056288  \n",
       "48841           1           0.122214  \n",
       "\n",
       "[48842 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#task 1\n",
    "# dropping table and making new csv dataframe lab_03_A.csv\n",
    "# dropped education, native-country, and sex\n",
    "\n",
    "dfNew = pd.read_csv(\"data\\lab_03_A.csv\")\n",
    "\n",
    "display(dfNew)\n",
    "\n",
    "dfNew.drop(columns=['education', 'native-country', 'sex', 'income'], errors='ignore', inplace=True)\n",
    "dfNew.to_csv(\"data\\lab_03_A.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08736902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2 _ dropped\n",
    "# 76% <= 50K => -1\n",
    "# 24% > 50K => 1\n",
    "\n",
    "y_dropped = dfNew['income-num']\n",
    "X_dropped = dfNew.drop(columns=['income-num'], errors='ignore')\n",
    "X_encoded = pd.get_dummies(X_dropped) #to turn categorical attributes to numerical attributes :)\n",
    "\n",
    "#stratify to ensure balanced class distribution between training, testing, and validation (since income is imbalanced) \n",
    "X_train_dropped, X_temp_dropped, y_train_dropped, y_temp_dropped = train_test_split(X_encoded, y_dropped, test_size=0.30, stratify=y_dropped, random_state=42)\n",
    "\n",
    "X_val_dropped, X_test_dropped, y_val_dropped, y_test_dropped = train_test_split(X_temp_dropped, y_temp_dropped, test_size=0.50, stratify=y_temp_dropped, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7711ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2 _ cleaned\n",
    "# 76% <= 50K => -1\n",
    "# 24% > 50K => 1\n",
    "\n",
    "y_cleaned = dfNew['income-num']\n",
    "X_cleaned = dfNew.drop(columns=['income-num'], errors='ignore')\n",
    "X_encoded = pd.get_dummies(X_cleaned) #to turn categorical attributes to numerical attributes :)\n",
    "\n",
    "#stratify to ensure balanced class distribution between training, testing, and validation (since income is imbalanced) \n",
    "X_train_cleaned, X_temp_cleaned, y_train_cleaned, y_temp_cleaned = train_test_split(X_encoded, y_cleaned, test_size=0.30, stratify=y_cleaned, random_state=42)\n",
    "\n",
    "X_val_cleaned, X_test_cleaned, y_val_cleaned, y_test_cleaned = train_test_split(X_temp_cleaned, y_temp_cleaned, test_size=0.50, stratify=y_temp_dropped, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a319ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 0.01, 'max_iter': 100, 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.97      0.88      5573\n",
      "           1       0.72      0.26      0.38      1753\n",
      "\n",
      "    accuracy                           0.80      7326\n",
      "   macro avg       0.77      0.62      0.63      7326\n",
      "weighted avg       0.79      0.80      0.76      7326\n",
      "\n",
      "Best Params: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.95      0.92      5573\n",
      "           1       0.81      0.61      0.70      1753\n",
      "\n",
      "    accuracy                           0.87      7326\n",
      "   macro avg       0.85      0.78      0.81      7326\n",
      "weighted avg       0.87      0.87      0.87      7326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task 3 _ dropped\n",
    "#linear regression lr\n",
    "#random forest rf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Hyperparameter grids from chatgpt\n",
    "param_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100, 200, 500]  # 💡 Added this\n",
    "}\n",
    "\n",
    "param_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]  # 💡 Added this\n",
    "}\n",
    "\n",
    "\n",
    "def train(model, param_grid):\n",
    "    grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train_dropped, y_train_dropped)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_val_dropped)\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(classification_report(y_val_dropped, y_pred))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "model_lr = train(LogisticRegression(), param_lr)\n",
    "\n",
    "model_rf = train(RandomForestClassifier(), param_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b310952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 0.01, 'max_iter': 100, 'solver': 'liblinear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.81      0.97      0.88      5573\n",
      "           1       0.72      0.26      0.38      1753\n",
      "\n",
      "    accuracy                           0.80      7326\n",
      "   macro avg       0.77      0.62      0.63      7326\n",
      "weighted avg       0.79      0.80      0.76      7326\n",
      "\n",
      "Best Params: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.95      0.92      5573\n",
      "           1       0.81      0.61      0.70      1753\n",
      "\n",
      "    accuracy                           0.87      7326\n",
      "   macro avg       0.85      0.78      0.81      7326\n",
      "weighted avg       0.87      0.87      0.87      7326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#task 3 _ cleaned\n",
    "#linear regression lr\n",
    "#random forest rf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Hyperparameter grids from chatgpt\n",
    "param_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100, 200, 500]  # 💡 Added this\n",
    "}\n",
    "\n",
    "param_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]  # 💡 Added this\n",
    "}\n",
    "\n",
    "\n",
    "def train(model, param_grid):\n",
    "    grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train_cleaned, y_train_cleaned)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_val_cleaned)\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(classification_report(y_val_cleaned, y_pred))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "model_lr = train(LogisticRegression(), param_lr)\n",
    "\n",
    "model_rf = train(RandomForestClassifier(), param_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a21cb",
   "metadata": {},
   "source": [
    "task 3 explanation\n",
    "\n",
    "logistic regression was used since we are doing binary classification over the income-num feature\n",
    "random forest since we have large dataset with high dimensionality and since the class income-num is imbalanced in out data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
